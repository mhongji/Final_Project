[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)\nlibrary(rsample)      # for data splitting\nlibrary(caret)        # for modeling & tuning\nlibrary(yardstick)    # for logLoss metric\nlibrary(here)\nlibrary(tidymodels)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(knitr)\n\n\n# load & prepare data\ndiabetes &lt;- read_csv(here(\"data\",\"diabetes_binary_health_indicators_BRFSS2015.csv\")) %&gt;%\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0,1), labels = c(\"No\",\"Yes\")),\n    PhysActivity    = factor(PhysActivity, levels = c(0,1), labels = c(\"No\",\"Yes\")),\n    HighBP          = factor(HighBP,     levels = c(0,1), labels = c(\"No\",\"Yes\")),\n    Sex             = factor(Sex,        levels = c(0,1), labels = c(\"Female\",\"Male\"))\n  )\n\n# reproducible split\nset.seed(123)\nsplit    &lt;- initial_split(diabetes, prop = 0.7, strata = Diabetes_binary)\ntrain_df &lt;- training(split)\ntest_df  &lt;- testing(split)\n\n# CV control using log‐loss\nctrl &lt;- trainControl(\n  method            = \"cv\",\n  number            = 5,\n  classProbs        = TRUE,\n  summaryFunction   = mnLogLoss,\n  savePredictions   = \"final\"\n)"
  },
  {
    "objectID": "Modeling.html#introduction-goals",
    "href": "Modeling.html#introduction-goals",
    "title": "Modeling",
    "section": "Introduction & Goals",
    "text": "Introduction & Goals\nWe aim to predict the binary outcome Diabetes_binary using three candidate algorithms:\n\nLogistic Regression (three different feature sets)\nClassification Tree\nRandom Forest\nModel selection will be based on 5‑fold cross‑validated log‐loss, and the final comparison will use the held‑out test set."
  },
  {
    "objectID": "Modeling.html#logistic-regression-models",
    "href": "Modeling.html#logistic-regression-models",
    "title": "Modeling",
    "section": "1. Logistic Regression Models",
    "text": "1. Logistic Regression Models"
  },
  {
    "objectID": "Modeling.html#why-logistic-regression",
    "href": "Modeling.html#why-logistic-regression",
    "title": "Modeling",
    "section": "Why logistic regression?",
    "text": "Why logistic regression?\nA logistic regression models the log‑odds of a binary outcome as a linear combination of predictors. It’s interpretable and often performs well on structured data.\n\nCandidate models\n\n# Define formulas \nformulas &lt;- list(\n  full      = Diabetes_binary ~ BMI + PhysActivity + HighBP + Sex,\n  reduced1  = Diabetes_binary ~ BMI + PhysActivity + HighBP,\n  interact1 = Diabetes_binary ~ BMI * HighBP + PhysActivity + Sex\n)\n\n# Train each logistic model \nlr_models &lt;- map(formulas, ~ train(\n  .x,                    \n  data      = train_df,\n  method    = \"glm\",\n  family    = binomial(),\n  metric    = \"logLoss\",\n  trControl = ctrl\n))\n\nnames(lr_models) &lt;- names(formulas)\n\n# Summarize CV log‑loss for each \nlr_results &lt;- map_dfr(\n  names(lr_models),\n  ~ {\n    m &lt;- lr_models[[.x]]\n    tibble(\n      model   = .x,\n      method  = m$method,\n      logLoss = min(m$results$logLoss)\n    )\n  }\n)\n\nprint(lr_results)\n\n# A tibble: 3 × 3\n  model     method logLoss\n  &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;\n1 full      glm      0.355\n2 reduced1  glm      0.355\n3 interact1 glm      0.355\n\n\nBest logistic model:\n\nbest_name &lt;- lr_results$model[which.min(lr_results$logLoss)]\nbest_lr   &lt;- lr_models[[best_name]]\n\ncat(\"Best logistic model is:\", best_name, \"\\n\")\n\nBest logistic model is: full \n\nprint(best_lr)\n\nGeneralized Linear Model \n\n177575 samples\n     4 predictor\n     2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142060, 142060, 142060, 142060, 142060 \nResampling results:\n\n  logLoss \n  0.354523\n\n\nThe full model (using all predictors) has the lowest log‑loss (around 0.32), so its probability estimates are the most accurate. We’ll use this full logistic model in our test‑set comparison against the tree and random forest."
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "Modeling",
    "section": "2. Classification Tree",
    "text": "2. Classification Tree\n\nWhat is a classification tree?\nA classification tree recursively splits the feature space into subsets that are increasingly “pure” in terms of the response. At each node it chooses the predictor and split‐point that minimize an impurity measure (e.g., Gini or entropy). Trees capture non‑linear relationships and interactions automatically, and the complexity parameter (cp) controls how aggressively the tree is pruned to avoid overfitting.\n\n# Grid of complexity parameters (cp) to try\ncp_grid &lt;- expand.grid(cp = seq(0.0005, 0.02, length = 10))\n\n# Train the tree model\ntree_mod &lt;- train(\n  Diabetes_binary ~ BMI + PhysActivity + HighBP + Sex,\n  data      = train_df,\n  method    = \"rpart\",\n  metric    = \"logLoss\",\n  tuneGrid  = cp_grid,\n  trControl = ctrl\n)\n\n# Show best cp and its CV log‑loss\ntree_mod$bestTune\n\n     cp\n10 0.02\n\nmin(tree_mod$results$logLoss)\n\n[1] 0.4037502\n\n\n\nOptimal cp: 0.02\n5‑fold CV log‑loss: ≈ 0.40\n\nThe pruned tree (cp = 0.02) fits the data but is less accurate than our full logistic model (log‑loss 0.32 vs  0.40). We’ll still include this tuned tree in our final test‑set comparison alongside the logistic regression and random forest."
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "Modeling",
    "section": "3. Random Forest",
    "text": "3. Random Forest\n\nWhat is a random forest?\nA random forest builds an ensemble of decision trees on bootstrap samples of the data, and at each split it considers only a random subset of predictors. Averaging many de‑correlated trees reduces variance and often improves generalization compared to a single tree, at the expense of interpretability.\n\n# Define the tuning grid for mtry (we have 4 predictors)\nrf_grid &lt;- expand.grid(mtry = 1:4)\n\n# Train the random forest with 500 trees and 5-fold CV on log-loss\nset.seed(456)\nrf_mod &lt;- train(\n  Diabetes_binary ~ BMI + PhysActivity + HighBP + Sex,\n  data      = train_df,\n  method    = \"rf\",\n  metric    = \"logLoss\",\n  tuneGrid  = rf_grid,\n  trControl = ctrl,    # 5-fold CV, classProbs=TRUE, summaryFunction=mnLogLoss\n  ntree     = 50      # number of trees in the forest\n)\n\n# Inspect the best mtry and its CV log-loss\nrf_mod$bestTune\n\n  mtry\n4    4\n\nmin(rf_mod$results$logLoss)\n\n[1] 4.444299\n\n\n\n# Define mtry grid for your 4 predictors\nrf_grid &lt;- expand.grid(mtry = 1:4)\n\n# Train the random forest (100 trees for speed)\nset.seed(123)\nrf_mod &lt;- train(\n  Diabetes_binary ~ BMI + PhysActivity + HighBP + Sex,\n  data      = train_df,\n  method    = \"rf\",\n  metric    = \"logLoss\",\n  tuneGrid  = rf_grid,\n  trControl = ctrl,\n  ntree     = 100\n)\n\n# Inspect CV results\nprint(rf_mod$results)\n\n  mtry  logLoss  logLossSD\n1    1 4.656831 0.06589774\n2    2 4.517512 0.01074452\n3    3 4.388299 0.03450324\n4    4 4.369988 0.04508828\n\ncat(\"Best mtry:\", rf_mod$bestTune$mtry, \n    \"→ CV log-loss =\", \n    min(rf_mod$results$logLoss), \"\\n\")\n\nBest mtry: 4 → CV log-loss = 4.369988 \n\n\nAs we increase mtry from 1 to 4, the CV log-loss steadily decrease, showing that using all four variables per split gives the lowest log-loss (around 4.37). However, these values remain far above the ~0.3–0.4 range of previous tree and logistic models, indicating the RF model is still underestimating diabetes probabilities."
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "Modeling",
    "section": "4. Final Model Selection",
    "text": "4. Final Model Selection\nNow we have three tuned models:\n\nLogistic Regression: best_lr\nClassification Tree: tree_mod\nRandom Forest: rf_mod\n\nNow that we have our three tuned models: best_lr, tree_mod, and rf_mod. We’ll compare them on the test set using log‑loss.\n\n# 1) Ensure “Yes” is the first (event) level\ntest_df2 &lt;- test_df %&gt;%\n  mutate(Diabetes_binary = fct_relevel(Diabetes_binary, \"Yes\"))\n\n# 2) Extract P(Yes) from each model explicitly\nlr_probs     &lt;- predict(best_lr,       test_df2, type = \"prob\")[, \"Yes\"]\ntree_probs   &lt;- predict(tree_mod,      test_df2, type = \"prob\")[, \"Yes\"]\nforest_probs &lt;- predict(rf_mod,        test_df2, type = \"prob\")[, \"Yes\"]\n\n# 3) Compute log-loss with mn_log_loss_vec()\ntest_results &lt;- tibble(\n  model   = c(\"Logistic\", \"Tree\", \"Forest\"),\n  logLoss = c(\n    mn_log_loss_vec(test_df2$Diabetes_binary, lr_probs),\n    mn_log_loss_vec(test_df2$Diabetes_binary, tree_probs),\n    mn_log_loss_vec(test_df2$Diabetes_binary, forest_probs)\n  )\n)\n\n# 4) Display\nkable(test_results)\n\n\n\n\nmodel\nlogLoss\n\n\n\n\nLogistic\n0.3543173\n\n\nTree\n0.4037523\n\n\nForest\n4.6252886\n\n\n\n\n\nSave the best model\n\n# Identify the overall winner\nbest_row   &lt;- test_results %&gt;% slice_min(logLoss, n = 1)\nbest_model &lt;- best_row$model\n\n# Save the chosen model\ndir.create(here(\"model\"), showWarnings = FALSE)\n\nif (best_model == \"Logistic\") {\n  saveRDS(best_lr,   here(\"model\",\"best_model.rds\"))\n} else if (best_model == \"Tree\") {\n  saveRDS(tree_mod,  here(\"model\",\"best_model.rds\"))\n} else if (best_model == \"Forest\") {\n  saveRDS(rf_mod,    here(\"model\",\"best_model.rds\"))\n}\n\n# Print which was saved\ncat(\"Saved the\", best_model, \"model as model/best_model.rds\\n\")\n\nSaved the Logistic model as model/best_model.rds"
  },
  {
    "objectID": "Modeling.html#conclusion",
    "href": "Modeling.html#conclusion",
    "title": "Modeling",
    "section": "5. Conclusion:",
    "text": "5. Conclusion:\nOn the held-out test set, the logistic regression achieves the lowest log-loss (≈ 0.35), meaning its probability estimates for diabetes are the most accurate. The decision tree comes next with a higher log-loss (≈ 0.40). The random forest is the worst (log-loss ≈ 4.63). In other words, the logistic model clearly outperforms both tree-based methods here, and the forest is badly underestimating diabetes risk."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/hongjingmao/Desktop/ST558/FinalProject/Final_Project\n\n# load data via relative path\ndiabetes &lt;- read_csv(here(\"data\",\"diabetes_binary_health_indicators_BRFSS2015.csv\"))\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(diabetes)\n\nRows: 253,680\nColumns: 22\n$ Diabetes_binary      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0…\n$ HighBP               &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1…\n$ HighChol             &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1…\n$ CholCheck            &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ BMI                  &lt;dbl&gt; 40, 25, 28, 27, 24, 25, 30, 25, 30, 24, 25, 34, 2…\n$ Smoker               &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0…\n$ Stroke               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ HeartDiseaseorAttack &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ PhysActivity         &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1…\n$ Fruits               &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1…\n$ Veggies              &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1…\n$ HvyAlcoholConsump    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ AnyHealthcare        &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ NoDocbcCost          &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ GenHlth              &lt;dbl&gt; 5, 3, 5, 2, 2, 2, 3, 3, 5, 2, 3, 3, 3, 4, 4, 2, 3…\n$ MentHlth             &lt;dbl&gt; 18, 0, 30, 0, 3, 0, 0, 0, 30, 0, 0, 0, 0, 0, 30, …\n$ PhysHlth             &lt;dbl&gt; 15, 0, 30, 0, 0, 2, 14, 0, 30, 0, 0, 30, 15, 0, 2…\n$ DiffWalk             &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0…\n$ Sex                  &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ Age                  &lt;dbl&gt; 9, 7, 9, 11, 11, 10, 9, 11, 9, 8, 13, 10, 7, 11, …\n$ Education            &lt;dbl&gt; 4, 6, 4, 3, 5, 6, 6, 4, 5, 4, 6, 5, 5, 4, 6, 6, 4…\n$ Income               &lt;dbl&gt; 3, 1, 8, 6, 4, 8, 7, 4, 1, 3, 8, 1, 7, 6, 2, 8, 3…\n\n# convert binaries → factors\nbinary_cols &lt;- c(\n  \"Diabetes_binary\",\"HighBP\",\"HighChol\",\"CholCheck\",\"Smoker\",\n  \"Stroke\",\"HeartDiseaseorAttack\",\"PhysActivity\",\"Fruits\",\"Veggies\",\n  \"HvyAlcoholConsump\",\"AnyHealthcare\",\"NoDocbcCost\",\"DiffWalk\"\n)\ndiabetes &lt;- diabetes %&gt;%\n  mutate(across(all_of(binary_cols),\n                ~ factor(.x, levels = c(0,1), labels = c(\"No\",\"Yes\"))),\n         Sex       = factor(Sex, levels = c(0,1), labels = c(\"Female\",\"Male\")),\n         Education = factor(Education))  # optional relabel\n\n# 4) missingness check\nmissing_summary &lt;- diabetes %&gt;%\n  summarise(across(everything(), ~ sum(is.na(.)))) %&gt;%\n  pivot_longer(everything(), names_to=\"var\", values_to=\"n_missing\")\n\nknitr::kable(missing_summary)\n\n\n\n\nvar\nn_missing\n\n\n\n\nDiabetes_binary\n0\n\n\nHighBP\n0\n\n\nHighChol\n0\n\n\nCholCheck\n0\n\n\nBMI\n0\n\n\nSmoker\n0\n\n\nStroke\n0\n\n\nHeartDiseaseorAttack\n0\n\n\nPhysActivity\n0\n\n\nFruits\n0\n\n\nVeggies\n0\n\n\nHvyAlcoholConsump\n0\n\n\nAnyHealthcare\n0\n\n\nNoDocbcCost\n0\n\n\nGenHlth\n0\n\n\nMentHlth\n0\n\n\nPhysHlth\n0\n\n\nDiffWalk\n0\n\n\nSex\n0\n\n\nAge\n0\n\n\nEducation\n0\n\n\nIncome\n0"
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "EDA",
    "section": "Introduction",
    "text": "Introduction\nThe BRFSS 2015 dataset contains survey responses from over 400,000 U.S. adults on lifestyle and health indicators.\n\nHere, Diabetes_binary (No/Yes) is our response; our ultimate goal is to build a predictive model via train/test split.\n\nThis EDA will:\n\nDescribe the data structure and check for missingness.\nJustify and explore three chosen predictors.\nPreview univariate and bivariate relationships to guide modeling."
  },
  {
    "objectID": "EDA.html#variable-selection-justification",
    "href": "EDA.html#variable-selection-justification",
    "title": "EDA",
    "section": "Variable Selection & Justification",
    "text": "Variable Selection & Justification\nI’ve chosen these three predictors:\n\nBMI (Body‐Mass Index): a key risk factor for type 2 diabetes (CDC, 2020).\nPhysActivity (Regular Physical Activity): exercise improves glucose metabolism.\nHighBP (High Blood Pressure): often comorbid with metabolic disorders."
  },
  {
    "objectID": "EDA.html#univariate-summaries",
    "href": "EDA.html#univariate-summaries",
    "title": "EDA",
    "section": "Univariate Summaries",
    "text": "Univariate Summaries\n\n1. Diabetes Prevalence\n\ndiabetes %&gt;%\n  count(Diabetes_binary) %&gt;%\n  mutate(pct = n/sum(n)) %&gt;%\n  ggplot(aes(x = Diabetes_binary, y = pct, fill = Diabetes_binary)) +\n    geom_col() +\n    scale_y_continuous(labels = scales::percent_format()) +\n    labs(title = \"Diabetes Prevalence in 2015\", y = \"Percent of Sample\")\n\n\n\n\n\n\n\n\nIn this sample, about 85% of respondents report no diabetes diagnosis in 2015, while roughly 15% report yes. This substantial class imbalance (≈6:1) means that a naive “always‑no” classifier would already be 85% accurate—so accuracy alone won’t be very informative. That’s why we’ll use log‑loss and stratified sampling in our cross‑validation to ensure our model learns to distinguish the minority class effectively.\n\n\n2. BMI Distribution\n\ndiabetes %&gt;%\n  ggplot(aes(x = BMI)) +\n    geom_histogram(bins = 50, fill = \"steelblue\", color = \"white\") +\n    labs(title = \"Distribution of BMI\", x = \"BMI\")\n\n\n\n\n\n\n\n# Compute BMI summary\nbmi_stats &lt;- diabetes %&gt;%\n  summarise(\n    n        = n(),\n    mean_BMI = mean(BMI, na.rm = TRUE),\n    med_BMI  = median(BMI, na.rm = TRUE),\n    sd_BMI   = sd(BMI, na.rm = TRUE),\n    min_BMI  = min(BMI, na.rm = TRUE),\n    max_BMI  = max(BMI, na.rm = TRUE)\n  )\n\nbmi_stats\n\n# A tibble: 1 × 6\n       n mean_BMI med_BMI sd_BMI min_BMI max_BMI\n   &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 253680     28.4      27   6.61      12      98\n\n\nThe BMI distribution is right‑skewed, with a mean of about 28.36 and a median of 27. Most values cluster between 22 and 35, indicating that a large portion of respondents are in the overweight (BMI 25–30) or obese (BMI &gt;30) categories. The long tail beyond 40 shows a smaller subset with very high BMI. Because higher BMI is a well–documented risk factor for type 2 diabetes, we expect this variable to be highly predictive in our models, and we may also consider transforming or binning it (e.g., normal vs. overweight vs. obese) if it improves model performance.\n\n\n3. Physical Activity\n\ndiabetes %&gt;%\n  count(PhysActivity) %&gt;%\n  mutate(pct = n/sum(n)) %&gt;%\n  ggplot(aes(x = PhysActivity, y = pct, fill = PhysActivity)) +\n    geom_col() +\n    scale_y_continuous(labels = scales::percent_format()) +\n    labs(title = \"Regular Physical Activity\", y = \"Percent Reporting\")\n\n\n\n\n\n\n\ndiabetes %&gt;%\n  count(PhysActivity) %&gt;%\n  mutate(pct = scales::percent(n / sum(n))) \n\n# A tibble: 2 × 3\n  PhysActivity      n pct  \n  &lt;fct&gt;         &lt;int&gt; &lt;chr&gt;\n1 No            61760 24%  \n2 Yes          191920 76%  \n\n\n76% of respondents report engaging in regular physical activity, while 24% do not. Since exercise is known to improve insulin sensitivity and help regulate blood sugar, we might expect the inactive group to show a higher diabetes rate. In our subsequent bivariate plots, we’ll check whether the “No” group indeed has a larger proportion of diabetes cases, which would confirm that physical activity is a useful predictor in our models.\n\n\n4. High Blood Pressure\n\ndiabetes %&gt;%\n  count(HighBP) %&gt;%\n  mutate(pct = n/sum(n)) %&gt;%\n  ggplot(aes(x = HighBP, y = pct, fill = HighBP)) +\n    geom_col() +\n    scale_y_continuous(labels = scales::percent_format()) +\n    labs(title = \"Self‑Reported Hypertension\", y = \"Percent Reporting\")\n\n\n\n\n\n\n\nhighbp_summary &lt;- diabetes %&gt;%\n  count(HighBP) %&gt;%\n  mutate(pct = scales::percent(n / sum(n)))\nhighbp_summary\n\n# A tibble: 2 × 3\n  HighBP      n pct  \n  &lt;fct&gt;   &lt;int&gt; &lt;chr&gt;\n1 No     144851 57%  \n2 Yes    108829 43%  \n\n\n43% of respondents report having high blood pressure, while 57% do not. Because hypertension and diabetes often co‑occur via shared cardiovascular and metabolic pathways, we expect the “Yes” group to exhibit a higher rate of diabetes. In the next section’s bivariate plot, we’ll confirm whether the hypertensive subgroup does indeed show a disproportionately large diabetes prevalence, underscoring HighBP as a potentially strong predictor."
  },
  {
    "objectID": "EDA.html#bivariate-relationships",
    "href": "EDA.html#bivariate-relationships",
    "title": "EDA",
    "section": "Bivariate Relationships",
    "text": "Bivariate Relationships\n\nBMI vs Diabetes\n\ndiabetes %&gt;%\n  ggplot(aes(x = BMI, color = Diabetes_binary)) +\n    geom_freqpoly(binwidth = 1) +\n    labs(title = \"BMI by Diabetes Status\", y = \"Count\")\n\n\n\n\n\n\n\ndiabetes %&gt;%\n  group_by(Diabetes_binary) %&gt;%\n  summarise(\n    mean_BMI   = mean(BMI, na.rm = TRUE),\n    median_BMI = median(BMI, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 3\n  Diabetes_binary mean_BMI median_BMI\n  &lt;fct&gt;              &lt;dbl&gt;      &lt;dbl&gt;\n1 No                  27.8         27\n2 Yes                 31.9         31\n\n\nThe blue line (Yes) is clearly shifted to the right compared to the salmon line (No), indicating that individuals with diabetes tend to have higher BMI values. For example, the mode of the non‑diabetic group is around a BMI of 25–26, whereas the diabetic group peaks closer to 27–28 and has a heavier tail extending into the obese range (BMI &gt; 30). This visual separation confirms that BMI is strongly associated with diabetes status and suggests it will be a valuable predictor in our models.\n\n\nPhysical Activity vs Diabetes\n\ndiabetes %&gt;%\n  ggplot(aes(x = PhysActivity, fill = Diabetes_binary)) +\n    geom_bar(position = \"fill\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    labs(title = \"Diabetes Rate by Activity\", y = \"Percent with Diabetes\")\n\n\n\n\n\n\n\ndiabetes %&gt;%\n  group_by(PhysActivity) %&gt;%\n  summarise(\n    diabetes_rate = mean(Diabetes_binary == \"Yes\") * 100\n  )\n\n# A tibble: 2 × 2\n  PhysActivity diabetes_rate\n  &lt;fct&gt;                &lt;dbl&gt;\n1 No                    21.1\n2 Yes                   11.6\n\n\nAmong respondents who report no regular physical activity, about 21% have diabetes, whereas only about 11% of those who do exercise report diabetes. This nearly two‐fold difference strongly suggests that physical activity is protective and will be an important predictor in our modeling pipeline.\n\n\nHigh BP vs Diabetes\n\ndiabetes %&gt;%\n  ggplot(aes(x = HighBP, fill = Diabetes_binary)) +\n    geom_bar(position = \"fill\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    labs(title = \"Diabetes Rate by Hypertension\", y = \"Percent with Diabetes\")\n\n\n\n\n\n\n\n# Compute diabetes rate by hypertension status\nbp_diabetes_rate &lt;- diabetes %&gt;%\n  group_by(HighBP) %&gt;%\n  summarise(\n    diabetes_rate = mean(Diabetes_binary == \"Yes\") * 100\n  )\n\nbp_diabetes_rate\n\n# A tibble: 2 × 2\n  HighBP diabetes_rate\n  &lt;fct&gt;          &lt;dbl&gt;\n1 No              6.04\n2 Yes            24.4 \n\n\nIndividuals without hypertension have only about a 6% diabetes prevalence, whereas those with high blood pressure show roughly a 24% prevalence. This near four‑fold increase strongly confirms that self‑reported hypertension is highly associated with diabetes. It also underscores why HighBP will be a key predictor in our modeling phase.\nClick here for the Modeling Page"
  }
]